

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction &mdash; Bioverse DL 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="installing.html" />
    <link rel="prev" title="Welcome to Bioverse DL’s documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/bioverse-white.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Project documentation</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="#deep-learning-architectures">Deep Learning architectures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">UNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#technique">Technique</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#image-segmentation">Image segmentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="installing.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="documentation.html">Documentation</a></li>
</ul>
<p class="caption"><span class="caption-text">Demo and examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="demo.html">Demo and Examples</a></li>
</ul>
<p class="caption"><span class="caption-text">TODO list</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="todo.html">TODO list</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Bioverse DL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Introduction</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/introduction.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<img alt="_images/bioverse.png" src="_images/bioverse.png" />
<p>The deep-learning module incorporate essential procedures to prepare remote sensing images mostly for automatic classification methods, in this case specifically, using deep-learning approaches. The sections below are organized to explain the main procedures implemented in the Bioverse DL module.</p>
</div>
<div class="section" id="deep-learning-architectures">
<h1>Deep Learning architectures<a class="headerlink" href="#deep-learning-architectures" title="Permalink to this headline">¶</a></h1>
<p>Beyond the routines, this code was prepared to have a personal use or even ready for adaptation to be run in server-side. The ain is to incorporate some of the main Deep Learning models for any remote sensing image analysis and mapping. In this version, the following DL architectures were tested:
- <a class="reference external" href="https://lmb.informatik.uni-freiburg.de/Publications/2015/RFB15a/">UNet</a></p>
<div class="section" id="id1">
<h2>UNet<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>The UNet is convolutional network architecture for fast and precise segmentation of digital images (<a class="reference external" href="https://arxiv.org/abs/1505.04597">UNet</a>). Until now, the “U shaped” Deep Learning architecture has outperformed the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks.</p>
<p>UNet, evolved from the traditional convolutional neural network, was first designed and applied in 2015 to process biomedical images, as the paper cited before. As a general convolutional neural network focuses its task on image classification, where input is an image and output is one label, but in biomedical cases, it requires us not only to distinguish whether there is a disease, but also to identify the area.</p>
<p>UNet is dedicated to solving this problem. The reason it is able to identify and delineate the boundaries in a process of a pixel-wise classification. In this sense, the input and output must to be the same size. For example, for an input image of size 2x2, we have the respective :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mi">255</span><span class="p">,</span> <span class="mi">230</span><span class="p">],</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">12</span><span class="p">]]</span>
</pre></div>
</div>
<p>where each number is a pixel. Thus, the output will have the same size of 2x2:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
</pre></div>
</div>
<p>where the outputs could be any number between 0 and 1. The UNet architecture has the following structure:</p>
<img alt="_images/unet.png" src="_images/unet.png" />
<p>The U shape is its characteristic. The architecture is symmetric and consists of two major parts: the left part is called contracting path, which is constituted by the general convolutional process; the right part is expansive path, which is constituted by transposed 2d convolutional layers (you can think it as an upsampling technic for now). Now let’s have a quick look at the implementation:</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>To write this section, the following sources have been consultanted:</dt><dd><ul class="simple">
<li></li>
</ul>
</dd>
</dl>
</div>
</div>
<div class="section" id="technique">
<h1>Technique<a class="headerlink" href="#technique" title="Permalink to this headline">¶</a></h1>
<p>The detection technique is composed by a set of image processing operators, mainly by Machine Learning techniques, which allow to explore the spectral, spatial and contextual properties of this species in a broad and emerging way. In addition, it is expected that the methodology has reasonable robustness and accuracy, therefore, the methodology is composed of scalable, interoperable, flexible and easily accessible architectures, allowing for any future modifications, experiments or replications.</p>
<div class="section" id="image-segmentation">
<h2>Image segmentation<a class="headerlink" href="#image-segmentation" title="Permalink to this headline">¶</a></h2>
<p>In order to classify an image pixel by pixel using a supervised approach, a collection of desired targets is usually needed. This dataset consists of many pairs of image (raster) and its respective label, which have the exact location of the desired objects in the image. These are then presented to a mathematical model that can understand this targets by analysing the possible patterns, spectral similarities, geometries, and many other characteristics that later is easily distinguishable over unknown images. This process is called training, and the module image-processing was developed to prepare the dataset using any satellite image (details of image-processing can be found here). Right after the training, the mathematical model choose can then be used as</p>
<p>The prediction or image segmentation procedure involve two types: (i) for images where the dimension is equal to the samples’s dimensions used during training, and (ii) images where the dimension is larger. Besides, the inferences have two classes of images, the images without any geographic information, and images with geographic information. The difference is that for images with no geographic metadata, the poligonization (the process to convert PNG prediction in SHP shapefiles - geographic vectors), will not be performed. In this section, we focus specifically how the (ii) was implemented.</p>
<p>Considering a large geographic image as an example, in the figure below is shown how the inference is made. First (a), the large image is tilled in a way that each tile have the same dimension as it was trained.</p>
<img alt="_images/buffer-prediction.png" src="_images/buffer-prediction.png" />
<p>In order to prevent discontinuous predictions between each tile, a buffer is applied (see (a)). The buffer can be configured also in <cite>settings.py</cite>, with the <cite>BUFFER_TO_INFERENCE</cite> variable, where the integer value represents the number of pixels to apply the buffering. In this way, zero will perform the inferences without buffering. The maximum buffering value is the half of each tile’s dimension.</p>
<p>After to predict, each tile will have a correspondent segmentation (see (b)). After to predict every single tile that compose the image, the predictions are then merged (c). Due to the buffering, the discontinuity is minimized during merging. Finally, getting a more consistent map in the end (d).</p>
<p>he predictions in PNG format will be placed in <cite>output_prediction</cite>. If it is a large image, then it will be place the tile’s predictions first in <cite>tmp_slices_predictions</cite>, then, the merging procedure will select all tiles and place the merged predictions in <cite>output_prediction</cite>. When done, the poligonization is performed (only for geographic files). The final vector file is place in <cite>output_prediction_shp</cite>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="installing.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to Bioverse DL’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Rodolfo Lotte

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>